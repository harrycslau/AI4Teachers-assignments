{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/harrycslau/AI4Teachers-assignments/blob/master/FML2w3_multilayer_perceptrons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "#### Part of the course on \"Foundations of machine learning\", Department of Mathematics and Statistics, University of Turku, Finland\n",
    "#### Lectures available on YouTube: https://youtube.com/playlist?list=PLbkSohdmxoVAZ9DEHEWHjeGK7Ei-DjKHI&si=Msu74_I0qhLrRWcu\n",
    "#### Code available on GitHub: https://github.com/ionpetre/FoundML_course_assignments\n",
    "\n",
    "#### This notebook is based on the following sources: \n",
    "\n",
    "> \n",
    "\n",
    "A Multilayer Perceptron (MLP) is a type of feedforward artificial neural network that consists of multiple layers of interconnected artificial neurons, or nodes. They can be used for both classification, as well as regression problems. We demonstrate them in this notebook in their simplest format, with a single hidden layer, in addition to the input and the output layers. \n",
    "\n",
    "In this noteebook we use the MNIST dataset to train an MLP classifier, and a World Health Organisation dataset on life expectancy in various countries to train an MLP regressor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-09T17:34:11.060373Z",
     "iopub.status.busy": "2021-09-09T17:34:11.059927Z",
     "iopub.status.idle": "2021-09-09T17:34:12.616363Z",
     "shell.execute_reply": "2021-09-09T17:34:12.615487Z",
     "shell.execute_reply.started": "2021-09-09T17:34:11.06034Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, average_precision_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the seed of the random number generator, for reproducibility purposes\n",
    "\n",
    "import os\n",
    "\n",
    "def reset_seed(SEED = 0):\n",
    "    \"\"\"Reset the seed for every random library in use (System, numpy)\"\"\"\n",
    "\n",
    "    os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "\n",
    "reset_seed(2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Demo MLP classifiers on the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The MNIST dataset: \n",
    "\n",
    "The MNIST dataset is a widely used dataset in the field of machine learning and computer vision. It consists of a collection of handwritten digits, and its name stands for the \"Modified National Institute of Standards and Technology\" dataset. \n",
    "\n",
    "The MNIST dataset contains a total of 70,000 grayscale images of the 0-9 handwritten digits. The images are 28x28 pixels. The MNIST dataset consists of a training set of 60,000 images and a test set of 10,000 images. The MNIST dataset is commonly used for tasks related to digit recognition, classification, and image analysis.\n",
    "It serves as a benchmark dataset for developing and evaluating machine learning algorithms and deep learning models.\n",
    "\n",
    "While MNIST is a well-known dataset, it is relatively simple compared to some real-world problems. Other datasets with more complex images exist, such as CIFAR-10, ImageNet, and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print('We have %2d training pictures and %2d test pictures.' % (X_train.shape[0],X_test.shape[0]))\n",
    "print('Each picture is of size (%2d,%2d)' % (X_train.shape[1], X_train.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some images\n",
    "\n",
    "def display_train_image(position):\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.title('Example %d. Label: %d' % (position, y_train[position]))\n",
    "    plt.imshow(X_train[position], cmap=plt.cm.gray_r)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "for i in range(50):\n",
    "    display_train_image(1200*i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the dataset balanced?\n",
    "\n",
    "y_train_count = np.unique(y_train, return_counts=True)\n",
    "df_y_train = pd.DataFrame({'Label':y_train_count[0], 'Count':y_train_count[1]})\n",
    "df_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    test_size=0.2, \n",
    "    random_state=150, \n",
    "    stratify=y_train,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Check the result of the data split\n",
    "\n",
    "print('# of training images:', X_train.shape[0])\n",
    "y_train_count = np.unique(y_train, return_counts=True)\n",
    "df_y_train = pd.DataFrame({'Label':y_train_count[0], 'Train samples':y_train_count[1]})\n",
    "print(df_y_train.to_string(index=False))\n",
    "\n",
    "print('# of validation images:', X_valid.shape[0])\n",
    "y_valid_count = np.unique(y_valid, return_counts=True)\n",
    "df_y_valid = pd.DataFrame({'Label':y_valid_count[0], 'Valid samples':y_valid_count[1]})\n",
    "print(df_y_valid.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise the data\n",
    "\n",
    "# Reshape the data so that each 28 x 28 picture is transformed into a 784-long vector\n",
    "X_train=X_train.reshape(X_train.shape[0],-1)\n",
    "print(\"Shape of the training data: \",X_train.shape)\n",
    "X_valid=X_valid.reshape(X_valid.shape[0],-1)\n",
    "X_test=X_test.reshape(X_test.shape[0],-1)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "minmax_scaler.fit(X_train)\n",
    "\n",
    "X_train_std = minmax_scaler.transform(X_train)\n",
    "X_valid_std = minmax_scaler.transform(X_valid)\n",
    "X_test_std  = minmax_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a multilayer perceptron classifier on the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "MLPclf = MLPClassifier(\n",
    "    hidden_layer_sizes=(10,), # number of neurons in the hidden layer\n",
    "    activation='logistic', \n",
    "    random_state=150, \n",
    "    max_iter=300,       # number of epochs   \n",
    "    batch_size=500,     # batch size for mini-batch training \n",
    "    learning_rate='constant', \n",
    "    learning_rate_init=0.001, \n",
    "    shuffle=True,\n",
    "    verbose=True,\n",
    "    early_stopping=False, \n",
    "    n_iter_no_change=10,\n",
    "    tol=1e-4,\n",
    "    solver='sgd',       # stochastic gradient descent\n",
    ")\n",
    "\n",
    "MLPclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of parameters in the model:\", \n",
    "      MLPclf.coefs_[0].size+len(MLPclf.intercepts_[0])+MLPclf.coefs_[1].size+len(MLPclf.intercepts_[1])\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the quality of the predictions through the accuracy score (on train and validation) and through the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = MLPclf.predict(X_train)\n",
    "y_valid_pred = MLPclf.predict(X_valid)\n",
    "\n",
    "print(\"The classification results on the training data:\")\n",
    "print(classification_report(y_train,y_train_pred))\n",
    "print(\"Confusion matrix (training data):\\n\", confusion_matrix(y_train,y_train_pred))\n",
    "\n",
    "print(\"\\n The classification results on the validation data:\")\n",
    "print(classification_report(y_valid,y_valid_pred))\n",
    "print(\"Confusion matrix (validation data):\\n\", confusion_matrix(y_valid,y_valid_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the learning curve per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_10_500_log = MLPclf.loss_curve_\n",
    "\n",
    "plt.plot(train_loss_10_500_log, color='green', alpha=0.8, label='10 hidden neurons, batch=500, logistic activation')\n",
    "plt.title(\"Loss over epochs\", fontsize=14)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try a few other setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPclf = MLPClassifier(\n",
    "    hidden_layer_sizes=(20,), # number of neurons in the hidden layer\n",
    "    activation='logistic', \n",
    "    random_state=150, \n",
    "    max_iter=300,       # number of epochs   \n",
    "    batch_size=300,     # batch size for mini-batch training \n",
    "    learning_rate='constant', \n",
    "    learning_rate_init=0.001, \n",
    "    shuffle=True,\n",
    "    verbose=True,\n",
    "    early_stopping=False, \n",
    "    n_iter_no_change=10,\n",
    "    tol=1e-4,\n",
    "    solver='sgd',       # stochastic gradient descent\n",
    ")\n",
    "\n",
    "MLPclf.fit(X_train, y_train)\n",
    "train_loss_20_300_log = MLPclf.loss_curve_\n",
    "\n",
    "\n",
    "MLPclf = MLPClassifier(\n",
    "    hidden_layer_sizes=(50,), # number of neurons in the hidden layer\n",
    "    activation='logistic', \n",
    "    random_state=150, \n",
    "    max_iter=300,       # number of epochs   \n",
    "    batch_size=1000,     # batch size for mini-batch training \n",
    "    learning_rate='constant', \n",
    "    learning_rate_init=0.001, \n",
    "    shuffle=True,\n",
    "    verbose=True,\n",
    "    early_stopping=False, \n",
    "    n_iter_no_change=10,\n",
    "    tol=1e-4,\n",
    "    solver='sgd',       # stochastic gradient descent\n",
    ")\n",
    "\n",
    "MLPclf.fit(X_train, y_train)\n",
    "train_loss_50_1000_log = MLPclf.loss_curve_\n",
    "\n",
    "\n",
    "MLPclf = MLPClassifier(\n",
    "    hidden_layer_sizes=(30,), # number of neurons in the hidden layer\n",
    "    activation='relu', \n",
    "    random_state=150, \n",
    "    max_iter=300,       # number of epochs   \n",
    "    batch_size=300,     # batch size for mini-batch training \n",
    "    learning_rate='constant', \n",
    "    learning_rate_init=0.001, \n",
    "    shuffle=True,\n",
    "    verbose=True,\n",
    "    early_stopping=False, \n",
    "    n_iter_no_change=10,\n",
    "    tol=1e-4,\n",
    "    solver='sgd',       # stochastic gradient descent\n",
    ")\n",
    "\n",
    "MLPclf.fit(X_train, y_train)\n",
    "train_loss_30_300_relu = MLPclf.loss_curve_\n",
    "\n",
    "\n",
    "MLPclf = MLPClassifier(\n",
    "    hidden_layer_sizes=(20,), # number of neurons in the hidden layer\n",
    "    activation='tanh', \n",
    "    random_state=150, \n",
    "    max_iter=300,       # number of epochs   \n",
    "    batch_size=300,     # batch size for mini-batch training \n",
    "    learning_rate='constant', \n",
    "    learning_rate_init=0.001, \n",
    "    shuffle=True,\n",
    "    verbose=True,\n",
    "    early_stopping=False, \n",
    "    n_iter_no_change=10,\n",
    "    tol=1e-4,\n",
    "    solver='sgd',       # stochastic gradient descent\n",
    ")\n",
    "\n",
    "MLPclf.fit(X_train, y_train)\n",
    "train_loss_20_300_tanh = MLPclf.loss_curve_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(train_loss_10_500_log, color='green', alpha=0.8, label='10 hidden neurons, batch=500, logistic activation')\n",
    "plt.plot(train_loss_20_300_log, color='red', alpha=0.8, label='20 hidden neurons, batch=300, logistic activation')\n",
    "plt.plot(train_loss_50_1000_log, color='blue', alpha=0.8, label='50 hidden neurons, batch=1000, logistic activation')\n",
    "plt.plot(train_loss_30_300_relu, color='orange', alpha=0.8, label='30 hidden neurons, batch=300, relu activation')\n",
    "plt.plot(train_loss_20_300_tanh, color='brown', alpha=0.8, label='20 hidden neurons, batch=300, tanh activation')\n",
    "plt.title(\"Loss over epochs\", fontsize=14)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "Different parameter setups can have quite a drastic effect on the performance of the MLP model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train an MLP with different combinations of parameters to find better setups. You can add more options to the combination.\n",
    "#### NOTE: each combination of parameters will get a new fit. Expect about one minute or so per fit on a \"standard\" computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "              \"hidden_layer_sizes\": [(10,), (20,)],\n",
    "              \"activation\": ['logistic', 'relu', 'tanh'],\n",
    "              \"batch_size\": [200, 500],\n",
    "              }\n",
    "\n",
    "MLPclf = MLPClassifier(\n",
    "    hidden_layer_sizes=(20,), #*\n",
    "    activation='logistic', #*\n",
    "    random_state=150, \n",
    "    max_iter=300,          \n",
    "    batch_size=200,      #*\n",
    "    learning_rate='constant', \n",
    "    learning_rate_init=0.001, \n",
    "    shuffle=True,\n",
    "    verbose=True,\n",
    "    early_stopping=False, \n",
    "    n_iter_no_change=10,\n",
    "    tol=1e-4,\n",
    "    solver='sgd', \n",
    ")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gcv = GridSearchCV(\n",
    "    estimator = MLPclf,\n",
    "    cv = 3,\n",
    "    param_grid = param_grid,\n",
    "    refit = True,\n",
    "    n_jobs = -1,\n",
    "    verbose = 1,\n",
    ")\n",
    "gcv.fit(X_train,y_train)\n",
    "\n",
    "pd.DataFrame(gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_MLPclf = gcv.best_estimator_\n",
    "\n",
    "print(\"Number of parameters in the model:\", \n",
    "      best_MLPclf.coefs_[0].size+len(best_MLPclf.intercepts_[0])+best_MLPclf.coefs_[1].size+len(best_MLPclf.intercepts_[1])\n",
    "     )\n",
    "\n",
    "y_train_pred = best_MLPclf.predict(X_train)\n",
    "y_valid_pred = best_MLPclf.predict(X_valid)\n",
    "y_test_pred = best_MLPclf.predict(X_test)\n",
    "\n",
    "print(\"The classification results on the training data:\")\n",
    "print(classification_report(y_train,y_train_pred))\n",
    "print(\"Confusion matrix (training data):\\n\", confusion_matrix(y_train,y_train_pred))\n",
    "\n",
    "print(\"\\n The classification results on the validation data:\")\n",
    "print(classification_report(y_valid,y_valid_pred))\n",
    "print(\"Confusion matrix (validation data):\\n\", confusion_matrix(y_valid,y_valid_pred))\n",
    "\n",
    "print(\"\\n The classification results on the test data:\")\n",
    "print(classification_report(y_test,y_test_pred))\n",
    "print(\"Confusion matrix (test data):\\n\", confusion_matrix(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "The MLP model got an accurracy of 0.93 on the test dataset, with very high precision and recall across all classes. It does suffer a little from the class imbalance: the perfoormance is slightly better for the larger classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_valid\n",
    "del X_test\n",
    "del y_train\n",
    "del y_valid\n",
    "del y_test\n",
    "del MLPclf\n",
    "del best_MLPclf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Life expectancy model\n",
    "\n",
    "We will train an MLP regression model to predict the life expectancy in a country, depending on a set of key factors in that country: infantile deaths, alcohol consumption, vaccination data, GDP, body mass index, schooling, and others. Some definitions of these factors can be found at https://datahelpdesk.worldbank.org/knowledgebase/topics/19286-world-development-indicators-wdi. \n",
    "\n",
    "The health data was collected 2000-2015 in 193 countries and made available under the Global Health Observatory (GHO) data repository of the World Health Organization (WHO). The economic data was collected from United Nation website. Several data files have been merged together into a single dataset. The dataset aims to help answer the following key questions:\n",
    "\n",
    ">Does the life expectancy reliably depend on the factors included in this dataset? \n",
    "\n",
    ">How much should a country having a lower life expectancy value (<65) increase its healthcare expenditure in order to improve its average lifespan?\n",
    "\n",
    ">Does life expectancy has positive or negative correlation with eating habits, lifestyle, exercise, smoking, drinking alcohol etc.\n",
    "\n",
    ">What is the impact of schooling on the lifespan of humans?\n",
    "\n",
    ">What is the impact of Immunization coverage on life expectancy?\n",
    "\n",
    "Data source: https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who (author: KUMARRAJARSHI). The data was collected from WHO and United Nations website with the help of Deeksha Russell and Duan Wang.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset\n",
    "For this challenge, you need to download the training and the test datasets from Moodle (or from the Kaggle source above) and make sure it is saved in the same folder as your code or indicate the relative folder location in the read function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"2015_WHO_Life_Expectancy_Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. How many features you have in the dataset?\n",
    "#### Q2. How many data points do you have in the dataset?\n",
    "#### Q3. Do you have missing values in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the categorical features\n",
    "\n",
    "categorical = [var for var in X.columns if X[var].dtype=='O']\n",
    "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
    "print('The categorical variables are :', categorical)\n",
    "\n",
    "# check for missing values in categorical variables \n",
    "\n",
    "X[categorical].isnull().sum()\n",
    "\n",
    "print(X['Status'].value_counts())\n",
    "print(X['Country'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will leave the categorical features encoded as they are, they will not be used in training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the numerical variables\n",
    "\n",
    "numerical = [var for var in X.columns if X[var].dtype!='O']\n",
    "print('There are {} numerical variables\\n'.format(len(numerical)))\n",
    "print('The numerical variables are :', numerical)\n",
    "\n",
    "# check missing values in numerical variables\n",
    "X[numerical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to remove all datapoints with at least one of the features missing\n",
    "\n",
    "X_complete = X.dropna(axis=0)\n",
    "print(X_complete.info())\n",
    "\n",
    "# We conclude that we lose about half of the data, on a dataset that is relatively small to start with.\n",
    "# We give up on this idea\n",
    "\n",
    "del X_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check some statistical indicators of the numerical features to decide how to handle the missing values\n",
    "\n",
    "X[numerical].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data curation decisions\n",
    "1. 10 values missing from life expectancy, which will be our target values. Drop the rows missing the values.\n",
    "2. The minimal values in all columns are 0 or very close to 0. This is unrealistic and it probably indicates empty entries encoded with such small number. This suggests that replacing empty values with 0 is in line with the way the data is already filled in. This is a big decision that in a more careful modeling scenario may have to be tested against other imputation techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows with empty life expectancy\n",
    "X = X[X['Life expectancy '].notna()]\n",
    "\n",
    "# Fill in the remaining empty values with 0\n",
    "X = X.fillna(0)\n",
    "\n",
    "print(X.info())\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the target variable from the dataset\n",
    "\n",
    "y = X['Life expectancy ']\n",
    "X = X.drop(['Life expectancy '], axis=1)\n",
    "\n",
    "# Update the list of the numerical variables\n",
    "numerical.remove(\"Life expectancy \")\n",
    "\n",
    "print(\"Number of numerical features to train on: \", len(numerical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design decisions\n",
    ">One question is whether the split should be stratified or not, and if yes, based on what column. One possibility would be to stratify it based on the developed/under development status of each country, except that the values in those columns seem strange (check for example the status for Finland). In this notebook we do an un-stratified split. \n",
    "\n",
    ">Another question is whether the split should be grouped, i.e., all data points from a single country to be placed together either in train or in validation. This is often done in medical setups, for example on patient data, to avoid data leakage in-between train and validation, due to correlations between the samples from the same patient. In this notebook we will not do a grouped split, but this may be an option to be tested in a more extensive analysis. \n",
    "\n",
    ">Another decision is whether to do a traditional train/validation/test design, or whether to run a cross-validation setup. The relatively small dataset indicates that a CV may be a good idea. For simplicity though, we implement a traditional train/validation/test setup here, but this may also be changed in a more extensive analysis. \n",
    "\n",
    ">We will have to standardize the (numerical features of the) data. We use StandardScaler to bring them to mean 0 and standard deviation 1. \n",
    "\n",
    ">The training will be done only on the numerical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train/validation/test\n",
    "\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    random_state=120, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_valid, \n",
    "    y_train_valid, \n",
    "    test_size=0.25, \n",
    "    random_state=120, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "del X_train_valid\n",
    "del y_train_valid\n",
    "\n",
    "# convert to pandas dataframe\n",
    "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_valid = pd.DataFrame(X_valid, columns=X.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_valid = pd.DataFrame(y_valid)\n",
    "y_test = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature scaling\n",
    "We use StandardScaler to get all numerical features with mean 0 and std 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training\n",
    "We train an MLP on the numerical features of X_train to predict the life expectancy variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same scaler to the validation data. \n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an MLP regression model\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import r2_score as R2\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "MLPreg = MLPRegressor(\n",
    "    hidden_layer_sizes=(20,), \n",
    "    activation='logistic', \n",
    "    random_state=150, \n",
    "    max_iter=1000,          \n",
    "    batch_size=20,      \n",
    "    learning_rate='constant', \n",
    "    learning_rate_init=0.001, \n",
    "    shuffle=True,\n",
    "    verbose=True,\n",
    "    early_stopping=False, \n",
    "    n_iter_no_change=10,\n",
    "    tol=1e-4,\n",
    "    solver='sgd', \n",
    ")\n",
    "\n",
    "MLPreg.fit(X_train[numerical], np.ravel(y_train))\n",
    "\n",
    "y_train_pred = MLPreg.predict(X_train[numerical])\n",
    "print(f'Validation MAE score {MAE(y_valid, y_valid_pred)}')\n",
    "print(f'Validation R2 score {R2(y_valid, y_valid_pred)}')\n",
    "\n",
    "# Your code here: number of parameters, performance on the validation data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the loss curve\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(MLPreg.loss_curve_, color='green', alpha=0.8, label='20 hidden neurons, batch=20, logistic activation')\n",
    "plt.title(\"Loss over epochs\", fontsize=14)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4. How many parameters does your model have? \n",
    "#### Q5. What is the R2 on the validation data (2 decimals)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R2 score indicates a great result, but the number of parameters is potentially large for the relatively small dataset we have. Let's train an MLP regressor with around 60 parameters. Keep the batch size the same (for reproducibility purposes). This will also be the smallest model with R2 on train at least 0.90. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a smaller MLP regression model\n",
    "\n",
    "smallMLPreg = MLPRegressor(\n",
    "    hidden_layer_sizes=(NNN,), \n",
    "    activation='logistic', \n",
    "    random_state=150, \n",
    "    max_iter=1000,          \n",
    "    batch_size=20,      \n",
    "    learning_rate='constant', \n",
    "    learning_rate_init=0.001, \n",
    "    shuffle=True,\n",
    "    verbose=True,\n",
    "    early_stopping=False, \n",
    "    n_iter_no_change=10,\n",
    "    tol=1e-4,\n",
    "    solver='sgd', \n",
    ")\n",
    "\n",
    "\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the loss curve\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(smallMLPreg.loss_curve_, color='green', alpha=0.8, label='MLP regressor for the life expectancy model')\n",
    "plt.title(\"Loss over epochs\", fontsize=14)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6. How many hidden neurons does the smaller model have? \n",
    "#### Q7. What is the R2 score of the smaller model on the validation data (2 decimals only)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the smaller model on the test data. Remember the standardisation of the data. \n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8. What is the R2 score of the smaller model on the test data (2 decimals only)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample code on how to use the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the prediction on Finland for year 2015.\n",
    "\n",
    "my_country = \"Finland\"\n",
    "my_year = 2015\n",
    "\n",
    "filter = (X['Country'] == my_country) & (X['Year']==my_year)\n",
    "\n",
    "my_datapoint = X[filter]\n",
    "y_true = y[X.index[filter]]\n",
    "\n",
    "print(\"My data:\")\n",
    "#display(my_datapoint)\n",
    "\n",
    "scaled_X = pd.DataFrame(scaler.transform(my_datapoint[numerical]), columns=X[numerical].columns)\n",
    "y_pred = smallMLPreg.predict(scaled_X)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {'Country': X[filter]['Country'],\n",
    "     'Year': X[filter]['Year'],\n",
    "     'True life exp':y_true, \n",
    "     'Predicted life exp': np.around(y_pred, decimals=1)\n",
    "    }\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the prediction on Finland over several years.\n",
    "\n",
    "my_country = \"Finland\"\n",
    "my_year = 2015\n",
    "\n",
    "filter = (X['Country'] == my_country) \n",
    "\n",
    "my_datapoint = X[filter]\n",
    "y_true = y[X.index[filter]]\n",
    "\n",
    "print(\"My data:\")\n",
    "#display(my_datapoint)\n",
    "\n",
    "scaled_X = pd.DataFrame(scaler.transform(my_datapoint[numerical]), columns=X[numerical].columns)\n",
    "y_pred = smallMLPreg.predict(scaled_X)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {'Country': X[filter]['Country'],\n",
    "     'Year': X[filter]['Year'],\n",
    "     'True life exp':y_true, \n",
    "     'Predicted life exp': np.around(y_pred, decimals=1)\n",
    "    }\n",
    ")\n",
    "\n",
    "#display(df)\n",
    "\n",
    "df.set_index('Year')[['True life exp', 'Predicted life exp']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

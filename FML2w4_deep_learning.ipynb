{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning with fully connected artificial neural networks\n",
    "#### Part of the course on \"Foundations of machine learning\", Department of Mathematics and Statistics, University of Turku, Finland\n",
    "#### Lectures available on YouTube: https://youtube.com/playlist?list=PLbkSohdmxoVAZ9DEHEWHjeGK7Ei-DjKHI&si=Msu74_I0qhLrRWcu\n",
    "#### Code available on GitHub: https://github.com/ionpetre/FoundML_course_assignments\n",
    "\n",
    "#### This notebook is partially based on the following sources: \n",
    "\n",
    "> https://www.tensorflow.org/tutorials/keras/classification\n",
    "\n",
    "We demonstrate in this notebook the use of fully connected neural networks for classification and regression. We use the tensorflow and keras as the deep learning Pyhton libraries. \n",
    "\n",
    "Datasets used in this notebook: Fashion MNIST, California housing, CIFAR-10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.tensorflow.org/tutorials/keras/classification:\n",
    "\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2017 Fran√ßois Chollet\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a\n",
    "# copy of this software and associated documentation files (the \"Software\"),\n",
    "# to deal in the Software without restriction, including without limitation\n",
    "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
    "# and/or sell copies of the Software, and to permit persons to whom the\n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
    "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
    "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
    "# DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the seed of the random number generator, for reproducibility purposes\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import set_random_seed\n",
    "\n",
    "def reset_seed(SEED = 0):\n",
    "\n",
    "    # Set the seed using keras.utils.set_random_seed. This will set:\n",
    "    # 1) `numpy` seed\n",
    "    # 2) `tensorflow` random seed\n",
    "    # 3) `python` random seed\n",
    "    set_random_seed(SEED)\n",
    "\n",
    "    # This will make TensorFlow ops as deterministic as possible, but it will\n",
    "    # affect the overall performance, so it's not enabled by default.\n",
    "    # `enable_op_determinism()` is introduced in TensorFlow 2.9.\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "\n",
    "reset_seed(2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Demo an fully connected neural network classifiers on the fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The fashion MNIST dataset: \n",
    "\n",
    "This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST.\n",
    "\n",
    "The classes are:\n",
    "\n",
    "| Label | Description   |\n",
    "|-------|---------------|\n",
    "|    0  | T-shirt/top   |\n",
    "|    1  |\tTrouser     |\n",
    "|    2  |\tPullover    |\n",
    "|    3  |\tDress       |\n",
    "|    4  |\tCoat        |\n",
    "|    5  |\tSandal      |\n",
    "|    6  |\tShirt       |\n",
    "|    7  |\tSneaker     |\n",
    "|    8  |\tBag         |\n",
    "|    9  |\tAnkle boot  |\n",
    "\n",
    "License: The copyright for Fashion-MNIST is held by Zalando SE. Fashion-MNIST is licensed under the MIT license.\n",
    "\n",
    "The data is available from the Keras datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(X_train_valid, y_train_valid), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "print('We have %2d training pictures and %2d test pictures.' % (X_train_valid.shape[0],X_test.shape[0]))\n",
    "print('Each picture is of size (%2d,%2d)' % (X_train_valid.shape[1], X_train_valid.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing\n",
    "The data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(X_train_valid[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data into [0,1] by dividing to 255\n",
    "\n",
    "X_train_valid_std = X_train_valid/255\n",
    "X_test_std  = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some images\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "for i in range(50):\n",
    "    plt.subplot(5,10,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_train_valid_std[i])\n",
    "    plt.xlabel(class_names[y_train_valid[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the dataset balanced?\n",
    "\n",
    "y_train_valid_count = np.unique(y_train_valid, return_counts=True)\n",
    "df_y_train_valid = pd.DataFrame({'Label':y_train_valid_count[0], 'Count':y_train_valid_count[1]})\n",
    "df_y_train_valid\n",
    "\n",
    "# A: YES!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - validation split\n",
    "\n",
    "X_train_std, X_valid_std, y_train, y_valid = train_test_split(\n",
    "    X_train_valid_std, \n",
    "    y_train_valid, \n",
    "    test_size=0.2, \n",
    "    random_state=150, \n",
    "    stratify=y_train_valid,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Check the result of the data split\n",
    "\n",
    "print('# of training images:', X_train_std.shape[0])\n",
    "print('# of validation images:', X_valid_std.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a fully connected neural network classifier on the fashion MNIST dataset\n",
    "\n",
    "We will use Keras, one of the most popular libraries for deep learning.\n",
    "Our network consists of a sequence of three `Dense` layers, with 128, 64, and 32 neurons, which are fully connected. We chose \"relu\" as the activation function. We also have an  10-neuron \"softmax\" output layer.\n",
    "\n",
    "Three more ingredients are to be chosen in the \"compilation\" phase of the model: \n",
    "* A loss function to quantify the current error of the model; \n",
    "* An optimizer: this is the mechanism through which the network will update itself based on the data it sees and its loss function.\n",
    "* Metrics to monitor during training and testing. Here we will only care about accuracy (the fraction of the images that were correctly classified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model can be setup by specifying each layer: \n",
    "#          its type, its size, its activation function.\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "ANNmodel = models.Sequential([\n",
    "    layers.Input(shape=(28, 28)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model must be compiled by specifying the numerical optimizer algorithm, \n",
    "#     the loss function, and metrics to be followed up epoch by epoch\n",
    "\n",
    "ANNmodel.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy(), \n",
    "             tf.keras.metrics.TruePositives(),\n",
    "            ],\n",
    ")\n",
    "\n",
    "ANNmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our model has 111 146 parameters, possibly quite many for the size of our dataset (48 000 images 28 x 28). Let's see how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels from numerical to categorical\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=10)\n",
    "y_valid_cat = to_categorical(y_valid, num_classes=10)\n",
    "y_test_cat = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We reset all variables implicitly instantiated by Keras/tensorflow\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# We reset the random number generators, for reproducibility purposes \n",
    "reset_seed(2023)\n",
    "\n",
    "\n",
    "# This callback will stop the training when there is no improvement in the loss \n",
    "#      for three consecutive epochs.\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "\n",
    "\n",
    "# Fit the model by specifying the number of epochs and the batch size\n",
    "# We also indicate the validation data so we can collect the evolution \n",
    "#      of the metrics through the epochs, both on train, as well as on validation.\n",
    "\n",
    "ANN_fit_history = ANNmodel.fit(X_train_std,\n",
    "                               y_train_cat, \n",
    "                               epochs=300, \n",
    "                               batch_size=128,\n",
    "                               callbacks=[callback],\n",
    "                               validation_data=(X_valid_std, y_valid_cat)\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the call to `model.fit()` returns a `History` object. This object has a member `history`, which is a dictionary containing data \n",
    "about everything that happened during training. Let's take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = ANN_fit_history.history\n",
    "print(history_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the evolution of the loss and the accurayc throughout the epochs\n",
    "# This is useful to find over-fitting and decide on early stopping of the training. \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "train_acc = history_dict['categorical_accuracy']\n",
    "val_acc = history_dict['val_categorical_accuracy']\n",
    "train_tp = np.array(history_dict['true_positives']) / X_train_std.shape[0]       # normalized true positives\n",
    "val_tp = np.array(history_dict['val_true_positives']) / X_valid_std.shape[0]     # normalized true positives \n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(epochs, train_loss, 'b', label='Training cat. cross-entropy')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation cat. cross-entropy')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(epochs, train_acc, 'b', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Categorical accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(epochs, train_tp, 'b', label='Training TP')\n",
    "plt.plot(epochs, val_tp, 'r', label='Validation TP')\n",
    "plt.title('Training and validation true positives')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('True positives')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see in the loss lines that the model is overfit: it learns well the training data, it starts well also on the validation data, but after a while the performance on validation gets bad. We can see the improvment on the training data and the stagnation on the validation data also in the accuracy and in the true positive rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict in the form of a 10-class probability distribution\n",
    "y_train_prob = ANNmodel.predict(X_train_std)\n",
    "\n",
    "# Select the most likely class\n",
    "y_train_pred=np.argmax(y_train_prob, axis=1)\n",
    "\n",
    "print(\"\\n The classification results on the train data:\")\n",
    "print(classification_report(y_train,y_train_pred))\n",
    "print(\"Confusion matrix (train data):\\n\", confusion_matrix(y_train,y_train_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The classification results for the validation data\n",
    "\n",
    "y_valid_prob = ANNmodel.predict(X_valid_std)\n",
    "y_valid_pred=np.argmax(y_valid_prob, axis=1)\n",
    "print(\"\\n The classification results on the validation data:\")\n",
    "print(classification_report(y_valid,y_valid_pred))\n",
    "print(\"Confusion matrix (validation data):\\n\", confusion_matrix(y_valid,y_valid_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise some of the predictions to see where the model is wrong.\n",
    "We display the correct prediction labels in blue and the incorrect prediction labels in red. The number gives the percentage (out of 100) for the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the first X validation images, their predicted labels, and the true labels in parenthesis.\n",
    "# Color correct predictions in blue and incorrect predictions in red.\n",
    "\n",
    "\n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    true_label, img = true_label[i], img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(img)\n",
    "\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "\n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    true_label = true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(10))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_rows = 10\n",
    "num_cols = 2\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(3*2*num_cols, 2*num_rows))\n",
    "\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "    plot_image(i, y_valid_prob[i], y_valid, X_valid_std)\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "    plot_value_array(i, y_valid_prob[i], y_valid)\n",
    "    plt.xticks(range(10), class_names, rotation=90)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's train a smaller model, hoping to get less overfit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We reset all variables implicitly instantiated by Keras/tensorflow \n",
    "#      (especially the internal names for layers and for the fit history)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# We reset the random number generators, for reproducibility purposes \n",
    "reset_seed(2023)\n",
    "\n",
    "\n",
    "\n",
    "# Only 2 smaller layers this time, plus the output layer\n",
    "# A drop from 111 146 parameters to 25 818 parameters.\n",
    "\n",
    "ANNmodel = models.Sequential([\n",
    "    layers.Input(shape=(28, 28)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# The model must be compiled by specifying the numerical optimizer algorithm, \n",
    "#     the loss function, and metrics to be followed up epoch by epoch\n",
    "\n",
    "ANNmodel.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy(), \n",
    "             tf.keras.metrics.TruePositives(),\n",
    "            ],\n",
    ")\n",
    "\n",
    "print(ANNmodel.summary())\n",
    "\n",
    "\n",
    "# This callback will stop the training when there is no improvement in the loss \n",
    "#      for three consecutive epochs.\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "\n",
    "# Fit the model by specifying the number of epochs and the batch size\n",
    "ANN_fit_history = ANNmodel.fit(X_train_std, \n",
    "                               y_train_cat, \n",
    "                               epochs=300, \n",
    "                               batch_size=128,\n",
    "                               callbacks=[callback],\n",
    "                               validation_data=(X_valid_std, y_valid_cat)\n",
    "                              )\n",
    "\n",
    "history_dict = ANN_fit_history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the evolution of the loss and the accurayc throughout the epochs\n",
    "# This is useful to find over-fitting and decide on early stopping of the training. \n",
    "\n",
    "print(history_dict.keys())\n",
    "\n",
    "train_loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "train_acc = history_dict['categorical_accuracy']\n",
    "val_acc = history_dict['val_categorical_accuracy']\n",
    "train_tp = np.array(history_dict['true_positives']) / X_train_std.shape[0]\n",
    "val_tp = np.array(history_dict['val_true_positives']) / X_valid_std.shape[0]\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(epochs, train_loss, 'b', label='Training cat. cross-entropy')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation cat. cross-entropy')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(epochs, train_acc, 'b', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Categorical accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(epochs, train_tp, 'b', label='Training TP')\n",
    "plt.plot(epochs, val_tp, 'r', label='Validation TP')\n",
    "plt.title('Training and validation true positives')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('True positives')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model remains overfit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the model to predict in the form of a 10-class probability distribution\n",
    "y_train_prob = ANNmodel.predict(X_train_std)\n",
    "\n",
    "# Select the most likely class\n",
    "y_train_pred=np.argmax(y_train_prob, axis=1)\n",
    "\n",
    "print(\"\\n The classification results on the train data:\")\n",
    "print(classification_report(y_train,y_train_pred))\n",
    "print(\"Confusion matrix (train data):\\n\", confusion_matrix(y_train,y_train_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The classification results for the validation data\n",
    "\n",
    "y_valid_prob = ANNmodel.predict(X_valid_std)\n",
    "y_valid_pred=np.argmax(y_valid_prob, axis=1)\n",
    "print(\"\\n The classification results on the validation data:\")\n",
    "print(classification_report(y_valid,y_valid_pred))\n",
    "print(\"Confusion matrix (validation data):\\n\", confusion_matrix(y_valid,y_valid_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the average accuracy is about the same for the smaller model as for the larger one. The model still seems overfit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The classification results for the test data\n",
    "\n",
    "y_test_prob = ANNmodel.predict(X_test_std)\n",
    "y_test_pred=np.argmax(y_test_prob, axis=1)\n",
    "print(\"\\n The classification results on the test data:\")\n",
    "print(classification_report(y_test,y_test_pred))\n",
    "print(\"Confusion matrix (test data):\\n\", confusion_matrix(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_valid\n",
    "del X_train_valid_std\n",
    "del X_train_std\n",
    "del X_valid_std\n",
    "del X_test\n",
    "del X_test_std\n",
    "del y_train\n",
    "del y_train_prob\n",
    "del y_train_pred\n",
    "del y_valid\n",
    "del y_valid_prob\n",
    "del y_valid_pred\n",
    "del y_test\n",
    "del y_test_prob\n",
    "del y_test_pred\n",
    "del ANNmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A deep learning regression model\n",
    "#### Data: the California housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset from sklearn, add the target to the main dataset\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "calif_X, calif_y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "display(calif_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train/validation/test\n",
    "\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(\n",
    "    calif_X, \n",
    "    calif_y, \n",
    "    test_size=0.2, \n",
    "    random_state=120, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_valid, \n",
    "    y_train_valid, \n",
    "    test_size=0.25, \n",
    "    random_state=120, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "del X_train_valid\n",
    "del y_train_valid\n",
    "\n",
    "# convert to pandas dataframe\n",
    "X_train = pd.DataFrame(X_train, columns=calif_X.columns)\n",
    "X_valid = pd.DataFrame(X_valid, columns=calif_X.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=calif_X.columns)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_valid = pd.DataFrame(y_valid)\n",
    "y_test = pd.DataFrame(y_test)\n",
    "\n",
    "del calif_X\n",
    "del calif_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise the data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "std_scaler.fit(X_train)\n",
    "\n",
    "X_train_std = std_scaler.transform(X_train)\n",
    "X_valid_std = std_scaler.transform(X_valid)\n",
    "X_test_std  = std_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reset all variables implicitly instantiated by Keras/tensorflow \n",
    "#      (especially the internal names for layers and for the fit history)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# We reset the random number generators, for reproducibility purposes \n",
    "reset_seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Design the model\n",
    "# For a regression model, the output layer will have a single neuron.\n",
    "# In the input layer we need to specify the input size. \n",
    "# The model has 3 dense layers of size 128/64/32 and a single neuron output layer. \n",
    "# This gives 11 521 parameters. \n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "ANNmodel = models.Sequential([\n",
    "    layers.Input(shape=(8,)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "# The model must be compiled by specifying the numerical optimizer algorithm, \n",
    "#     the loss function, and metrics to be followed up epoch by epoch\n",
    "\n",
    "ANNmodel.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=[\n",
    "       # tf.keras.metrics.MeanAbsoluteError(),\n",
    "       # tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "        tf.keras.metrics.R2Score(),\n",
    "        ],\n",
    ")\n",
    "\n",
    "print(ANNmodel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This callback will stop the training when there is no improvement in the loss \n",
    "#      for three consecutive epochs.\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "\n",
    "# Fit the model by specifying the number of epochs and the batch size\n",
    "ANN_fit_history = ANNmodel.fit(X_train_std, \n",
    "                               y_train, \n",
    "                               epochs=100, \n",
    "                               batch_size=32,\n",
    "                               callbacks=[callback],\n",
    "                               validation_data=(X_valid_std, y_valid)\n",
    "                              )\n",
    "\n",
    "history_dict = ANN_fit_history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the evolution of the loss and the accurayc throughout the epochs\n",
    "# This is useful to find over-fitting and decide on early stopping of the training. \n",
    "\n",
    "print(history_dict.keys())\n",
    "\n",
    "train_loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "#train_mape = history_dict['mean_absolute_percentage_error']\n",
    "#val_mape = history_dict['val_mean_absolute_percentage_error']\n",
    "train_R2 = history_dict['r2_score']\n",
    "val_R2 = history_dict['val_r2_score']\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, train_loss, 'b', label='Training MSE')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation MSE')\n",
    "plt.title('Train and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, train_R2, 'b', label='Training R2')\n",
    "plt.plot(epochs, val_R2, 'r', label='Validation R2')\n",
    "plt.title('R2 scores')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Train and validation R2 scores')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Looking at the plots it looks like it is overfit. So we need a smaller model. \n",
    "\n",
    "> Looking at the mean absolute percentage value, the model does not do well even on the train data. This may reflect the relatively limited dataset: too few features, possibly too few datapoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reset all variables implicitly instantiated by Keras/tensorflow \n",
    "#      (especially the internal names for layers and for the fit history)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# We reset the random number generators, for reproducibility purposes \n",
    "reset_seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design the model\n",
    "# The model has 2 dense layers of size 16/8 and a single neuron output layer. \n",
    "# This gives 289 parameters. \n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "ANNmodel = models.Sequential([\n",
    "    layers.Input(shape=(8,)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# The model must be compiled by specifying the numerical optimizer algorithm, \n",
    "#     the loss function, and metrics to be followed up epoch by epoch\n",
    "\n",
    "ANNmodel.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=[\n",
    "       # tf.keras.metrics.MeanAbsoluteError(),\n",
    "       # tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "        tf.keras.metrics.R2Score(),\n",
    "        ],\n",
    ")\n",
    "\n",
    "print(ANNmodel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This callback will stop the training when there is no improvement in the loss \n",
    "#      for three consecutive epochs.\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='r2_score', patience=10)\n",
    "\n",
    "\n",
    "# Fit the model by specifying the number of epochs and the batch size\n",
    "ANN_fit_history = ANNmodel.fit(X_train_std, \n",
    "                               y_train, \n",
    "                               epochs=100, \n",
    "                               batch_size=32,\n",
    "                               callbacks=[callback],\n",
    "                               validation_data=(X_valid_std, y_valid)\n",
    "                              )\n",
    "\n",
    "history_dict = ANN_fit_history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the evolution of the loss and the accurayc throughout the epochs\n",
    "# This is useful to find over-fitting and decide on early stopping of the training. \n",
    "\n",
    "print(history_dict.keys())\n",
    "\n",
    "train_loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "#train_mape = history_dict['mean_absolute_percentage_error']\n",
    "#val_mape = history_dict['val_mean_absolute_percentage_error']\n",
    "train_R2 = history_dict['r2_score']\n",
    "val_R2 = history_dict['val_r2_score']\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, train_loss, 'b', label='Training MSE')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation MSE')\n",
    "plt.title('Train and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, train_R2, 'b', label='Training R2')\n",
    "plt.plot(epochs, val_R2, 'r', label='Validation R2')\n",
    "plt.title('R2 scores')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Train and validation R2 scores')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Much better: the model is no longer overfit. It has learned great, R2 could be higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "def regression_results(y_true, y_pred):\n",
    "\n",
    "    # Regression metrics\n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    mae=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mape=metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2=metrics.r2_score(y_true, y_pred)\n",
    "    \n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('MAE: ', round(mae,4))\n",
    "    print('MAPE: ', round(mape,4))\n",
    "    print('R2: ', round(r2,4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The regression results for the test data\n",
    "\n",
    "y_test_pred = ANNmodel.predict(X_test_std)\n",
    "print(\"Regression results on the test dataset:\")\n",
    "regression_results(y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: The CIFAR-10 dataset\n",
    "#### Note: This is a notoriously difficult dataset to learn using fully connected neural networks. Let's see how well we can learn it!\n",
    "\n",
    "The CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The 10 classes are:\n",
    "\n",
    "| Label | Description   |\n",
    "|-------|---------------|\n",
    "|    0  | Airplane   |\n",
    "|    1  |\tAutomobile     |\n",
    "|    2  |\tBird    |\n",
    "|    3  |\tCat       |\n",
    "|    4  |\tDeer        |\n",
    "|    5  |\tDog      |\n",
    "|    6  |\tFrog       |\n",
    "|    7  |\tHorse     |\n",
    "|    8  |\tShip         |\n",
    "|    9  |\tTruck  |\n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. \n",
    "\n",
    "Webpage, including download: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "Dataset on Keras: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reset all variables implicitly instantiated by Keras/tensorflow \n",
    "#      (especially the internal names for layers and for the fit history)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# We reset the random number generators, for reproducibility purposes \n",
    "reset_seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(X_train_valid, y_train_valid), (X_test, y_test) = cifar10.load_data()\n",
    "y_train_valid = y_train_valid.ravel()\n",
    "y_test = y_test.ravel()\n",
    "\n",
    "print('We have %2d training pictures and %2d test pictures.' % (X_train_valid.shape[0],X_test.shape[0]))\n",
    "print('Each picture is of size (%2d,%2d)' % (X_train_valid.shape[1], X_train_valid.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data into [0,1] by dividing to 255\n",
    "\n",
    "X_train_valid_std = X_train_valid/255\n",
    "X_test_std  = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some images\n",
    "\n",
    "class_names = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "for i in range(50):\n",
    "    plt.subplot(5,10,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_train_valid_std[i])\n",
    "    plt.xlabel(class_names[int(y_train_valid[i])])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - validation split \n",
    "\n",
    "X_train_std, X_valid_std, y_train, y_valid = train_test_split(\n",
    "    X_train_valid_std, \n",
    "    y_train_valid, \n",
    "    test_size=0.2, \n",
    "    random_state=150, \n",
    "    stratify=y_train_valid,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Check the result of the data split\n",
    "\n",
    "print('# of training images:', X_train_std.shape[0])\n",
    "print('# of validation images:', X_valid_std.shape[0])\n",
    "print(\"Note the shape of the data (3 color channels):\", X_train_std.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels from numerical to categorical\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=10)\n",
    "y_valid_cat = to_categorical(y_valid, num_classes=10)\n",
    "y_test_cat = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an ANN model with an input \"Flatten\" layer of shape (32, 32, 3), accounting for the 3 color channels,\n",
    "#       followed by 3 layers of size 128/64/32, followed by an output layer of a suitable size.\n",
    "# Choose 'relu' for the activation function of the hidden layers, and a suitable activation for the output layer. \n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. How many (trainable) parameters does your 128/64/32 model have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We reset all variables implicitly instantiated by Keras/tensorflow\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# We reset the random number generators, for reproducibility purposes \n",
    "reset_seed(2023)\n",
    "\n",
    "\n",
    "# This callback will stop the training when there is no improvement in the loss \n",
    "#      for three consecutive epochs.\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "# Fit the model by specifying the number of epochs and the batch size\n",
    "# We also indicate the validation data so we can collect the evolution \n",
    "#      of the metrics through the epochs, both on train, as well as on validation.\n",
    "\n",
    "ANN_fit_history = ANNmodel.fit(X_train_std,\n",
    "                               y_train_cat, \n",
    "                               epochs=300, \n",
    "                               batch_size=128,\n",
    "                               callbacks=[callback],\n",
    "                               validation_data=(X_valid_std, y_valid_cat)\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = ANN_fit_history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "# Plot the evolution of the loss and the accurayc throughout the epochs\n",
    "# This is useful to find over-fitting and decide on early stopping of the training. \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "train_acc = history_dict['categorical_accuracy']\n",
    "val_acc = history_dict['val_categorical_accuracy']\n",
    "train_tp = np.array(history_dict['true_positives']) / X_train_std.shape[0]\n",
    "val_tp = np.array(history_dict['val_true_positives']) / X_valid_std.shape[0]\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(epochs, train_loss, 'b', label='Training cat. cross-entropy')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation cat. cross-entropy')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(epochs, train_acc, 'b', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Categorical accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(epochs, train_tp, 'b', label='Training TP')\n",
    "plt.plot(epochs, val_tp, 'r', label='Validation TP')\n",
    "plt.title('Training and validation true positives')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('True positives')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To think about: Based on the evolution of the loss throughout the epochs, do you consider the 128/64/32 model overfit (is the loss on the validation clearly increasing)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the accuracy of the 128/64/32 model on the training and on the validation data: run the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the model to predict in the form of a 10-class probability distribution\n",
    "y_train_prob = ANNmodel.predict(X_train_std)\n",
    "\n",
    "# Select the most likely class\n",
    "y_train_pred=np.argmax(y_train_prob, axis=1)\n",
    "\n",
    "print(\"\\n The classification results on the train data:\")\n",
    "print(classification_report(y_train,y_train_pred))\n",
    "print(\"Confusion matrix (train data):\\n\", confusion_matrix(y_train,y_train_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The classification results for the validation data\n",
    "\n",
    "y_valid_prob = ANNmodel.predict(X_valid_std)\n",
    "y_valid_pred=np.argmax(y_valid_prob, axis=1)\n",
    "print(\"\\n The classification results on the validation data:\")\n",
    "print(classification_report(y_valid,y_valid_pred))\n",
    "print(\"Confusion matrix (validation data):\\n\", confusion_matrix(y_valid,y_valid_pred))\n",
    "\n",
    "\n",
    "# The classification results for the test data\n",
    "\n",
    "y_test_prob = ANNmodel.predict(X_test_std)\n",
    "y_test_pred=np.argmax(y_test_prob, axis=1)\n",
    "print(\"\\n The classification results on the test data:\")\n",
    "print(classification_report(y_test,y_test_pred))\n",
    "print(\"Confusion matrix (test data):\\n\", confusion_matrix(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2. What is the accuracy of the 128/64/32 model on the training data? \n",
    "#### Q3. What is the accuracy of the 128/64/32 model on the validation data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the first X validation images, their predicted labels, and the true labels in parenthesis.\n",
    "# Color correct predictions in blue and incorrect predictions in red.\n",
    "\n",
    "\n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    true_label, img = int(true_label[i]), img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(img)\n",
    "\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "\n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    true_label = int(true_label[i])\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(10))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_rows = 10\n",
    "num_cols = 2\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(3*2*num_cols, 2*num_rows))\n",
    "\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "    plot_image(i, y_valid_prob[i], y_valid, X_valid_std)\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "    plot_value_array(i, y_valid_prob[i], y_valid)\n",
    "    plt.xticks(range(10), class_names, rotation=90)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a smaller model. \n",
    "# Use an input \"Flatten\" layer of shape (32, 32, 3), accounting for the 3 color channels,\n",
    "#       followed by 2 layers of size 64/32, followed by an output layer of a suitable size.\n",
    "# Choose 'relu' for the activation function of the hidden layers, and a suitable activation for the output layer. \n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4. How many (trainable) parameters does the 64/32 model have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We reset all variables implicitly instantiated by Keras/tensorflow\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# We reset the random number generators, for reproducibility purposes \n",
    "reset_seed(2023)\n",
    "\n",
    "\n",
    "# This callback will stop the training when there is no improvement in the loss \n",
    "#      for three consecutive epochs.\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "# Fit the model by specifying the number of epochs and the batch size\n",
    "# We also indicate the validation data so we can collect the evolution \n",
    "#      of the metrics through the epochs, both on train, as well as on validation.\n",
    "\n",
    "ANN_fit_history = ANNmodel.fit(X_train_std,\n",
    "                               y_train_cat, \n",
    "                               epochs=300, \n",
    "                               batch_size=128,\n",
    "                               callbacks=[callback],\n",
    "                               validation_data=(X_valid_std, y_valid_cat)\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = ANN_fit_history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "# Plot the evolution of the loss and the accurayc throughout the epochs\n",
    "# This is useful to find over-fitting and decide on early stopping of the training. \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "train_acc = history_dict['categorical_accuracy']\n",
    "val_acc = history_dict['val_categorical_accuracy']\n",
    "train_tp = np.array(history_dict['true_positives']) / X_train_std.shape[0]\n",
    "val_tp = np.array(history_dict['val_true_positives']) / X_valid_std.shape[0]\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(epochs, train_loss, 'b', label='Training cat. cross-entropy')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation cat. cross-entropy')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(epochs, train_acc, 'b', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Categorical accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(epochs, train_tp, 'b', label='Training TP')\n",
    "plt.plot(epochs, val_tp, 'r', label='Validation TP')\n",
    "plt.title('Training and validation true positives')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('True positives')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To think about: Based on the evolution of the loss throughout the epochs, do you consider the 64/32 model overfit (is the loss on the validation clearly increasing)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the accuracy of the 64/32 model on the training and on the validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5. What is the accuracy of the 64/32 model on the training data? \n",
    "#### Q6. What is the accuracy of the 64/32 model on the validation data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

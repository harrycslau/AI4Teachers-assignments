{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning with convolutional neural networks\n",
    "#### Part of the course on \"Foundations of machine learning\", Department of Mathematics and Statistics, University of Turku, Finland\n",
    "#### Lectures available on YouTube: https://youtube.com/playlist?list=PLbkSohdmxoVAZ9DEHEWHjeGK7Ei-DjKHI&si=Msu74_I0qhLrRWcu\n",
    "#### Code available on GitHub: https://github.com/ionpetre/FoundML_course_assignments\n",
    "\n",
    "#### This notebook is partially based on the following sources: \n",
    "\n",
    "> https://www.tensorflow.org/tutorials/keras/classification\n",
    "\n",
    "We demonstrate in this notebook the use of convolutional neural networks for classification. We use the tensorflow and keras as the Pyhton libraries. We also demonstrate the use of a pre-trained ResNet model. \n",
    "\n",
    "Datasets used in this notebook: Fashion MNIST, CIFAR-10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.tensorflow.org/tutorials/keras/classification:\n",
    "\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2017 FranÃ§ois Chollet\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a\n",
    "# copy of this software and associated documentation files (the \"Software\"),\n",
    "# to deal in the Software without restriction, including without limitation\n",
    "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
    "# and/or sell copies of the Software, and to permit persons to whom the\n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
    "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
    "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
    "# DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the seed of the random number generator, for reproducibility purposes\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import set_random_seed\n",
    "\n",
    "def reset_seed(SEED = 0):\n",
    "\n",
    "    # Set the seed using keras.utils.set_random_seed. This will set:\n",
    "    # 1) `numpy` seed\n",
    "    # 2) `tensorflow` random seed\n",
    "    # 3) `python` random seed\n",
    "    set_random_seed(SEED)\n",
    "\n",
    "    # This will make TensorFlow ops as deterministic as possible, but it will\n",
    "    # affect the overall performance, so it's not enabled by default.\n",
    "    # `enable_op_determinism()` is introduced in TensorFlow 2.9.\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "\n",
    "reset_seed(2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo a convolutional neural network classifier on the MNIST dataset\n",
    "We use the LeNet-5 architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "(X_train_valid, y_train_valid), (X_test, y_test) = mnist.load_data()\n",
    "img_width = X_train_valid.shape[1]\n",
    "img_heights = X_train_valid.shape[2]\n",
    "print('The size of our training dataset: samples x width x height x channels =', X_train_valid.shape)\n",
    "\n",
    "# Reshape the data to add an extra dimension for the grayscale 'color' channel\n",
    "# Depending on the version of Keras, the color channel is either expected in fromnt, or in the end\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train_valid = X_train_valid.reshape(X_train_valid.shape[0], 1, img_width, img_heights)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_width, img_heights)\n",
    "    input_shape = (1, img_width, img_heights)\n",
    "else:\n",
    "    X_train_valid = X_train_valid.reshape(X_train_valid.shape[0], img_width, img_heights, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_width, img_heights, 1)\n",
    "    input_shape = (img_width, img_heights, 1)\n",
    "\n",
    "print('The size of our training dataset: samples x width x height x channels =', X_train_valid.shape)\n",
    "print('We train on ',X_train_valid.shape[0], 'samples.')\n",
    "print('We test on ', X_test.shape[0], 'samples.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing\n",
    "The data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(X_train_valid[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data into [0,1] by dividing to 255\n",
    "\n",
    "X_train_valid_std = X_train_valid/255\n",
    "X_test_std  = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_valid_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input size: 32x32 expected in LeNet-5, we have 28x28\n",
    "The LeNet architecture accepts a 32x32 pixel images as input, mnist data is 28x28 pixels. We simply pad the images with zeros to overcome that. Another possibility is to modify the input layer size in the LeNet architecture. We prefer to leave that unchaged for historical reasons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad images with 0s\n",
    "X_train_valid_std = np.pad(X_train_valid_std, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "X_test_std = np.pad(X_test_std, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "    \n",
    "print(\"Updated Image Shape: {}\".format(X_train_valid_std.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some images\n",
    "\n",
    "class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "for i in range(50):\n",
    "    plt.subplot(5,10,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_train_valid_std[i])\n",
    "    plt.xlabel(class_names[y_train_valid[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - validation split\n",
    "\n",
    "X_train_std, X_valid_std, y_train, y_valid = train_test_split(\n",
    "    X_train_valid_std, \n",
    "    y_train_valid, \n",
    "    test_size=0.2, \n",
    "    random_state=150, \n",
    "    stratify=y_train_valid,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Check the result of the data split\n",
    "\n",
    "print('# of training images:', X_train_std.shape[0])\n",
    "print('# of validation images:', X_valid_std.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LeNet-5 implementation\n",
    "\n",
    "![lenet.png](https://raw.githubusercontent.com/MostafaGazar/mobile-ml/master/files/lenet.png)\n",
    ">>> LeNet-5 Architecture. Credit: [LeCun et al., 1998](http://yann.lecun.com/exdb/publis/psgz/lecun-98.ps.gz)\n",
    "\n",
    "#### Input\n",
    "    32x32x1 pixels image\n",
    "\n",
    "#### Architecture\n",
    "* **Convolutional #1** outputs 28x28x6\n",
    "    * **Activation** `relu`\n",
    "\n",
    "* **Pooling #1** The output shape should be 14x14x6.\n",
    "\n",
    "* **Convolutional #2** outputs 10x10x16.\n",
    "    * **Activation** `relu`\n",
    "\n",
    "* **Pooling #2** outputs 5x5x16.\n",
    "    * **Flatten** Flatten the output shape of the final pooling layer\n",
    "\n",
    "* **Fully Connected #1** outputs 120\n",
    "    * **Activation** `relu`\n",
    "\n",
    "* **Fully Connected #2** outputs 84\n",
    "    * **Activation** `relu`\n",
    "\n",
    "* **Fully Connected (Logits) #3** output 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model can be setup by specifying each layer: \n",
    "#          its type, its size, its activation function.\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "LeNet5model = models.Sequential([\n",
    "    layers.Input(shape=(32,32,1)),\n",
    "    layers.Conv2D(filters=6, \n",
    "                  kernel_size=(5, 5), \n",
    "                  strides=(1,1), \n",
    "                  padding='valid', \n",
    "                  activation='sigmoid', \n",
    "                 ), \n",
    "    layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(filters=16, \n",
    "                  kernel_size=(5, 5), \n",
    "                  strides=[1,1],\n",
    "                  padding='valid',\n",
    "                  activation='sigmoid'\n",
    "                 ),\n",
    "    layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(units=120, activation='sigmoid'),\n",
    "    layers.Dense(units=84, activation='sigmoid'),\n",
    "    layers.Dense(units=10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LeNet5model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our model has 61 706 parameters. Let's see how it trains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model must be compiled by specifying the numerical optimizer algorithm, \n",
    "#     the loss function, and metrics to be followed up epoch by epoch\n",
    "\n",
    "LeNet5model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy(), \n",
    "             tf.keras.metrics.TruePositives(),\n",
    "            ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels from numerical to categorical\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=10)\n",
    "y_valid_cat = to_categorical(y_valid, num_classes=10)\n",
    "y_test_cat = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We reset all variables implicitly instantiated by Keras/tensorflow\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# We reset the random number generators, for reproducibility purposes \n",
    "reset_seed(2023)\n",
    "\n",
    "\n",
    "# This callback will stop the training when there is no improvement in the loss \n",
    "#      for three consecutive epochs.\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "\n",
    "\n",
    "# Fit the model by specifying the number of epochs and the batch size\n",
    "# We also indicate the validation data so we can collect the evolution \n",
    "#      of the metrics through the epochs, both on train, as well as on validation.\n",
    "\n",
    "LeNet5_fit_history = LeNet5model.fit(X_train_std,\n",
    "                               y_train_cat, \n",
    "                               epochs=100, \n",
    "                               batch_size=500,\n",
    "                               callbacks=[callback],\n",
    "                               validation_data=(X_valid_std, y_valid_cat)\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the call to `model.fit()` returns a `History` object. This object has a member `history`, which is a dictionary containing data \n",
    "about everything that happened during training. Let's take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = LeNet5_fit_history.history\n",
    "print(history_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the evolution of the loss and the accurayc throughout the epochs\n",
    "# This is useful to find over-fitting and decide on early stopping of the training. \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "train_acc = history_dict['categorical_accuracy']\n",
    "val_acc = history_dict['val_categorical_accuracy']\n",
    "train_tp = np.array(history_dict['true_positives']) / X_train_std.shape[0]       # normalized true positives\n",
    "val_tp = np.array(history_dict['val_true_positives']) / X_valid_std.shape[0]     # normalized true positives \n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(epochs, train_loss, 'b', label='Training cat. cross-entropy')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation cat. cross-entropy')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(epochs, train_acc, 'b', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Categorical accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(epochs, train_tp, 'b', label='Training TP')\n",
    "plt.plot(epochs, val_tp, 'r', label='Validation TP')\n",
    "plt.title('Training and validation true positives')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('True positives')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is an amazing result, both on the training set, as well as on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict in the form of a 10-class probability distribution\n",
    "y_train_prob = LeNet5model.predict(X_train_std)\n",
    "\n",
    "# Select the most likely class\n",
    "y_train_pred=np.argmax(y_train_prob, axis=1)\n",
    "\n",
    "print(\"\\n The classification results on the train data:\")\n",
    "print(classification_report(y_train,y_train_pred))\n",
    "print(\"Confusion matrix (train data):\\n\", confusion_matrix(y_train,y_train_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The classification results for the validation data\n",
    "\n",
    "y_valid_prob = LeNet5model.predict(X_valid_std)\n",
    "y_valid_pred=np.argmax(y_valid_prob, axis=1)\n",
    "print(\"\\n The classification results on the validation data:\")\n",
    "print(classification_report(y_valid,y_valid_pred))\n",
    "print(\"Confusion matrix (validation data):\\n\", confusion_matrix(y_valid,y_valid_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The classification results for the test data\n",
    "\n",
    "y_test_prob = LeNet5model.predict(X_test_std)\n",
    "y_test_pred=np.argmax(y_test_prob, axis=1)\n",
    "print(\"\\n The classification results on the test data:\")\n",
    "print(classification_report(y_test,y_test_pred))\n",
    "print(\"Confusion matrix (test data):\\n\", confusion_matrix(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_valid\n",
    "del X_train_valid_std\n",
    "del X_train_std\n",
    "del X_valid_std\n",
    "del X_test\n",
    "del X_test_std\n",
    "del y_train\n",
    "del y_train_prob\n",
    "del y_train_pred\n",
    "del y_valid\n",
    "del y_valid_prob\n",
    "del y_valid_pred\n",
    "del y_test\n",
    "del y_test_prob\n",
    "del y_test_pred\n",
    "del LeNet5model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: train a convolutional neural network classifier on the fashion MNIST dataset\n",
    "We use the LeNet-5 architecture, originally introduced to classify the MNIST dataset. \n",
    "\n",
    "> Use the 28 x 28 original images in the fashion MNIST dataset, skip the padding to size 32x32. Instead, indicate in the input layer that the input size is (28,28,1).\n",
    "\n",
    "> Scale the data by diving to 255. \n",
    "\n",
    "> Use the LeNet-5 architecture demonstrated in this notebook for the MNIST dataset, modified so that it uses 'relu' instead of the 'sigmoid' activation and the MaxPooling2D layer instead of the AveragePooling2D, with the same parameters. \n",
    "\n",
    "> Reset the tensorflow environment and the random number generator seed in exactly the same way as for the MNIST dataset (this is important for reproducibility).\n",
    "\n",
    "> Train the model for 100 epochs with the same callback function we used before (the 'patience' parameter remains set to 3). Use a batch size of 500. \n",
    "\n",
    "> Measure the categorical accuracy on train, validation, and test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions \n",
    "> Q1. How many (trainable) parameters does your model have? A: \n",
    "\n",
    "> Q1. What is the accuracy of your LeNet-5 model on the training dataset? A: \n",
    "\n",
    "> Q2. What is the accuracy of your LeNet-5 model on the validation dataset? A: \n",
    "\n",
    "> Q1. What is the accuracy of your LeNet-5 model on the test dataset? A: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The fashion MNIST dataset: \n",
    "\n",
    "This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST.\n",
    "\n",
    "The classes are:\n",
    "\n",
    "| Label | Description   |\n",
    "|-------|---------------|\n",
    "|    0  | T-shirt/top   |\n",
    "|    1  |\tTrouser     |\n",
    "|    2  |\tPullover    |\n",
    "|    3  |\tDress       |\n",
    "|    4  |\tCoat        |\n",
    "|    5  |\tSandal      |\n",
    "|    6  |\tShirt       |\n",
    "|    7  |\tSneaker     |\n",
    "|    8  |\tBag         |\n",
    "|    9  |\tAnkle boot  |\n",
    "\n",
    "License: The copyright for Fashion-MNIST is held by Zalando SE. Fashion-MNIST is licensed under the MIT license.\n",
    "\n",
    "The data is available from the Keras datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "(X_train_valid, y_train_valid), (X_test, y_test) = fashion_mnist.load_data()\n",
    "img_width = X_train_valid.shape[1]\n",
    "img_heights = X_train_valid.shape[2]\n",
    "print('The size of our training dataset: samples x width x height x channels =', X_train_valid.shape)\n",
    "\n",
    "# Reshape the data to add an extra dimension for the grayscale 'color' channel\n",
    "# Depending on the version of Keras, the color channel is either expected in fromnt, or in the end\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train_valid = X_train_valid.reshape(X_train_valid.shape[0], 1, img_width, img_heights)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_width, img_heights)\n",
    "    input_shape = (1, img_width, img_heights)\n",
    "else:\n",
    "    X_train_valid = X_train_valid.reshape(X_train_valid.shape[0], img_width, img_heights, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_width, img_heights, 1)\n",
    "    input_shape = (img_width, img_heights, 1)\n",
    "\n",
    "print('The size of our training dataset: samples x width x height x channels =', X_train_valid.shape)\n",
    "print('We train on ',X_train_valid.shape[0], 'samples.')\n",
    "print('We test on ', X_test.shape[0], 'samples.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data into [0,1] by dividing to 255\n",
    "\n",
    "X_train_valid_std = X_train_valid/255\n",
    "X_test_std  = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display some images\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "for i in range(50):\n",
    "    plt.subplot(5,10,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_train_valid_std[i])\n",
    "    plt.xlabel(class_names[y_train_valid[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - validation split\n",
    "\n",
    "X_train_std, X_valid_std, y_train, y_valid = train_test_split(\n",
    "    X_train_valid_std, \n",
    "    y_train_valid, \n",
    "    test_size=0.2, \n",
    "    random_state=150, \n",
    "    stratify=y_train_valid,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Check the result of the data split\n",
    "\n",
    "print('# of training images:', X_train_std.shape[0])\n",
    "print('# of validation images:', X_valid_std.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The model can be setup by specifying each layer: \n",
    "#          its type, its size, its activation function.\n",
    "\n",
    "# Your code here: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: How many (trainable) parameters does your LeNet-5 model for Fashion MNIST have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model must be compiled by specifying the numerical optimizer algorithm, \n",
    "#     the loss function, and metrics to be followed up epoch by epoch\n",
    "\n",
    "# Your code here: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels from numerical to categorical\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=10)\n",
    "y_valid_cat = to_categorical(y_valid, num_classes=10)\n",
    "y_test_cat = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We reset all variables implicitly instantiated by Keras/tensorflow\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# We reset the random number generators, for reproducibility purposes \n",
    "reset_seed(2023)\n",
    "\n",
    "\n",
    "# This callback will stop the training when there is no improvement in the loss \n",
    "#      for three consecutive epochs.\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "\n",
    "\n",
    "# Fit the model by specifying the number of epochs and the batch size\n",
    "# We also indicate the validation data so we can collect the evolution \n",
    "#      of the metrics through the epochs, both on train, as well as on validation.\n",
    "\n",
    "# Your code here: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the call to `model.fit()` returns a `History` object. This object has a member `history`, which is a dictionary containing data \n",
    "about everything that happened during training. Let's take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = LeNet5_fit_history.history\n",
    "print(history_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the evolution of the loss and the accurayc throughout the epochs\n",
    "# This is useful to find over-fitting and decide on early stopping of the training. \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "train_acc = history_dict['categorical_accuracy']\n",
    "val_acc = history_dict['val_categorical_accuracy']\n",
    "train_tp = np.array(history_dict['true_positives']) / X_train_std.shape[0]       # normalized true positives\n",
    "val_tp = np.array(history_dict['val_true_positives']) / X_valid_std.shape[0]     # normalized true positives \n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(epochs, train_loss, 'b', label='Training cat. cross-entropy')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation cat. cross-entropy')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(epochs, train_acc, 'b', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Categorical accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(epochs, train_tp, 'b', label='Training TP')\n",
    "plt.plot(epochs, val_tp, 'r', label='Validation TP')\n",
    "plt.title('Training and validation true positives')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('True positives')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the model to predict in the form of a 10-class probability distribution\n",
    "y_train_prob = LeNet5model.predict(X_train_std)\n",
    "\n",
    "# Select the most likely class\n",
    "y_train_pred=np.argmax(y_train_prob, axis=1)\n",
    "\n",
    "print(\"\\n The classification results on the train data:\")\n",
    "print(classification_report(y_train,y_train_pred))\n",
    "print(\"Confusion matrix (train data):\\n\", confusion_matrix(y_train,y_train_pred))\n",
    "\n",
    "\n",
    "# The classification results for the validation data\n",
    "\n",
    "y_valid_prob = LeNet5model.predict(X_valid_std)\n",
    "y_valid_pred=np.argmax(y_valid_prob, axis=1)\n",
    "print(\"\\n The classification results on the validation data:\")\n",
    "print(classification_report(y_valid,y_valid_pred))\n",
    "print(\"Confusion matrix (validation data):\\n\", confusion_matrix(y_valid,y_valid_pred))\n",
    "\n",
    "\n",
    "\n",
    "# The classification results for the test data\n",
    "\n",
    "y_test_prob = LeNet5model.predict(X_test_std)\n",
    "y_test_pred=np.argmax(y_test_prob, axis=1)\n",
    "print(\"\\n The classification results on the test data:\")\n",
    "print(classification_report(y_test,y_test_pred))\n",
    "print(\"Confusion matrix (test data):\\n\", confusion_matrix(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2: What is the accuracy of your LeNet-5 model on the training Fashion MNIST dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3: What is the accuracy of your LeNet-5 model on the validation Fashion MNIST dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A4: What is the accuracy of your LeNet-5 model on the test Fashion MNIST dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise some of the predictions to see where the model is wrong.\n",
    "We display the correct prediction labels in blue and the incorrect prediction labels in red. The number gives the percentage (out of 100) for the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the first X validation images, their predicted labels, and the true labels in parenthesis.\n",
    "# Color correct predictions in blue and incorrect predictions in red.\n",
    "\n",
    "\n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    true_label, img = true_label[i], img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(img)\n",
    "\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "\n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    true_label = true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(10))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_rows = 10\n",
    "num_cols = 2\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(3*2*num_cols, 2*num_rows))\n",
    "\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "    plot_image(i, y_valid_prob[i], y_valid, X_valid_std)\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "    plot_value_array(i, y_valid_prob[i], y_valid)\n",
    "    plt.xticks(range(10), class_names, rotation=90)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_valid\n",
    "del X_train_valid_std\n",
    "del X_train_std\n",
    "del X_valid_std\n",
    "del X_test\n",
    "del X_test_std\n",
    "del y_train\n",
    "del y_train_prob\n",
    "del y_train_pred\n",
    "del y_valid\n",
    "del y_valid_prob\n",
    "del y_valid_pred\n",
    "del y_test\n",
    "del y_test_prob\n",
    "del y_test_pred\n",
    "del LeNet5model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CIFAR-10 dataset\n",
    "\n",
    "The CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The 10 classes are:\n",
    "\n",
    "| Label | Description   |\n",
    "|-------|---------------|\n",
    "|    0  | Airplane   |\n",
    "|    1  |\tAutomobile     |\n",
    "|    2  |\tBird    |\n",
    "|    3  |\tCat       |\n",
    "|    4  |\tDeer        |\n",
    "|    5  |\tDog      |\n",
    "|    6  |\tFrog       |\n",
    "|    7  |\tHorse     |\n",
    "|    8  |\tShip         |\n",
    "|    9  |\tTruck  |\n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. \n",
    "\n",
    "Webpage, including download: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "Dataset on Keras: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reset all variables implicitly instantiated by Keras/tensorflow \n",
    "#      (especially the internal names for layers and for the fit history)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# We reset the random number generators, for reproducibility purposes \n",
    "reset_seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(X_train_valid, y_train_valid), (X_test, y_test) = cifar10.load_data()\n",
    "y_train_valid = y_train_valid.ravel()\n",
    "\n",
    "print('We have %2d training pictures and %2d test pictures.' % (X_train_valid.shape[0],X_test.shape[0]))\n",
    "print('Each picture is of size (%2d,%2d)' % (X_train_valid.shape[1], X_train_valid.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data into [0,1] by dividing to 255\n",
    "\n",
    "X_train_valid_std = X_train_valid/255\n",
    "X_test_std  = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display some images\n",
    "\n",
    "class_names = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "for i in range(50):\n",
    "    plt.subplot(5,10,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_train_valid_std[i])\n",
    "    plt.xlabel(class_names[int(y_train_valid[i])])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - validation split \n",
    "\n",
    "X_train_std, X_valid_std, y_train, y_valid = train_test_split(\n",
    "    X_train_valid_std, \n",
    "    y_train_valid, \n",
    "    test_size=0.2, \n",
    "    random_state=150, \n",
    "    stratify=y_train_valid,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Check the result of the data split\n",
    "\n",
    "print('# of training images:', X_train_std.shape[0])\n",
    "print('# of validation images:', X_valid_std.shape[0])\n",
    "print(\"Note the shape of the data (3 color channels):\", X_train_std.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels from numerical to categorical\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=10)\n",
    "y_valid_cat = to_categorical(y_valid, num_classes=10)\n",
    "y_test_cat = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train a CNN model with an input layer of shape (32, 32, 3), accounting for the 3 color channels.\n",
    "# Try the LeNet-5 architecture. \n",
    "\n",
    "LeNet5model = models.Sequential([\n",
    "    layers.Input(shape=(32,32,3)),\n",
    "    layers.Conv2D(filters=6, \n",
    "                  kernel_size=(5, 5), \n",
    "                  strides=(1,1), \n",
    "                  padding='valid', \n",
    "                  activation='relu', \n",
    "                 ), \n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(filters=16, \n",
    "                  kernel_size=(5, 5), \n",
    "                  strides=[1,1],\n",
    "                  padding='valid',\n",
    "                  activation='relu'\n",
    "                 ),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(units=120, activation='relu'),\n",
    "    layers.Dense(units=84, activation='relu'),\n",
    "    layers.Dense(units=10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "LeNet5model.summary()\n",
    "\n",
    "LeNet5model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy(), \n",
    "             tf.keras.metrics.TruePositives(),\n",
    "            ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We reset all variables implicitly instantiated by Keras/tensorflow\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# We reset the random number generators, for reproducibility purposes \n",
    "reset_seed(2023)\n",
    "\n",
    "\n",
    "# This callback will stop the training when there is no improvement in the loss \n",
    "#      for three consecutive epochs.\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "# Fit the model by specifying the number of epochs and the batch size\n",
    "# We also indicate the validation data so we can collect the evolution \n",
    "#      of the metrics through the epochs, both on train, as well as on validation.\n",
    "\n",
    "LeNet5_fit_history = LeNet5model.fit(X_train_std,\n",
    "                               y_train_cat, \n",
    "                               epochs=100, \n",
    "                               batch_size=500,\n",
    "                               callbacks=[callback],\n",
    "                               validation_data=(X_valid_std, y_valid_cat)\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = LeNet5_fit_history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "# Plot the evolution of the loss and the accurayc throughout the epochs\n",
    "# This is useful to find over-fitting and decide on early stopping of the training. \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "train_acc = history_dict['categorical_accuracy']\n",
    "val_acc = history_dict['val_categorical_accuracy']\n",
    "train_tp = np.array(history_dict['true_positives']) / X_train_std.shape[0]\n",
    "val_tp = np.array(history_dict['val_true_positives']) / X_valid_std.shape[0]\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(epochs, train_loss, 'b', label='Training cat. cross-entropy')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation cat. cross-entropy')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(epochs, train_acc, 'b', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Categorical accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(epochs, train_tp, 'b', label='Training TP')\n",
    "plt.plot(epochs, val_tp, 'r', label='Validation TP')\n",
    "plt.title('Training and validation true positives')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('True positives')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the model to predict in the form of a 10-class probability distribution\n",
    "# Get the predictions for train, validation and test in the form of the confusion matrices. \n",
    "\n",
    "# The classification results for the train data\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The classification results for the validation data\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "# The classification results for the test data\n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5: How many trainable parameters does your CNN model for CIFAR-10 have?\n",
    "#### Q6: What is the accuracy of your CNN model for CIFAR-10 on the training set?\n",
    "#### Q7: What is the accuracy of your CNN model for CIFAR-10 on the validation set?\n",
    "#### Q8: What is the accuracy of your CNN model for CIFAR-10 on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the first X validation images, their predicted labels, and the true labels in parenthesis.\n",
    "# Color correct predictions in blue and incorrect predictions in red.\n",
    "\n",
    "\n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    true_label, img = int(true_label[i]), img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(img)\n",
    "\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "\n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    true_label = int(true_label[i])\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(10))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_rows = 10\n",
    "num_cols = 2\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(3*2*num_cols, 2*num_rows))\n",
    "\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "    plot_image(i, y_valid_prob[i], y_valid, X_valid_std)\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "    plot_value_array(i, y_valid_prob[i], y_valid)\n",
    "    plt.xticks(range(10), class_names, rotation=90)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra challenge: train a CNN model with at least 80% accuracy on the validation dataset\n",
    "#### Warning: the model below takes a long time to train, unless some accelerator is used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNmodel3 = tf.keras.models.Sequential([\n",
    "    layers.Conv2D(filters=32, kernel_size=3, input_shape=X_train_std.shape[1:], activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    #layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    #layers.BatchNormalization(),\n",
    "    #layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    #layers.BatchNormalization(),\n",
    "    #layers.MaxPool2D(pool_size=(2, 2)),\n",
    "    #layers.Dropout(0.25),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
